# Hi, Iâ€™m Chi ğŸ‘‹

**System Developer (Telecom) + Industrial PhD (Software Engineering / AI)** in Sweden.  
I build production systems and research **how to evaluate and verify LLM-generated artifacts** (text, code, structured specs) with **reproducible pipelines**.

- ğŸ’¼ Work: performance & capacity, reliability/observability, tooling & automation
- ğŸ“ Research: LLM output evaluation, validity gates, metrics, structured artifacts (requirements, UML)
- ğŸ¤ Open to: applied SE+AI collaboration, benchmarking & evaluation discussions, industryâ€“academia projects

**Contact**
- Email: xiao0600 [at] gmail.com
- ORCID: https://orcid.org/0009-0005-1562-0637

---

## â­ Featured

### uml-llm-eval-2025 â€” Reproducible LLM Artifact Evaluation
A reproducible evaluation pipeline for LLM-generated UML/structured artifacts across models and domains.  
Repo: https://github.com/chi0600/uml-llm-eval-2025

### fraud-anomaly-ds â€” Fraud / Anomaly Detection (Credit Card Transactions)
End-to-end fraud detection under severe class imbalance, focusing on PR-AUC and precisionâ€“recall trade-offs.  
Repo: https://github.com/chi0600/fraud-anomaly-ds

### uml-sequence-diagram-dataset â€” UML Sequence Diagram Dataset (ICSE-SEIP 2025)
Original public dataset/artifacts for "UML Sequence Diagram Generation: A Multi-Model, Multi-Domain Evaluation".  
Repo: https://github.com/chi0600/uml-sequence-diagram-dataset Â· DOI: https://doi.org/10.1109/ICSE-SEIP66354.2025.00030

---

## ğŸ§© What I Do

### System Development
- Build maintainable components and workflows
- Performance/capacity analysis and engineering trade-offs
- Reliability, observability, and automation (CI/scripts)

### Industrial PhD Research
- Artifact-level evaluation: validity gates, oracle/judge design, metrics
- Cross-domain evaluation pipelines for LLM outputs
- Reproducible experiments and reporting


---

## ğŸ›  Tech
Python Â· Git Â· Docker Â· Linux Â· GitHub Actions Â· LaTeX  
LLM tooling: RAG, embeddings, evaluation harnesses

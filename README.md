# Hi, Iâ€™m Chi Xiao ğŸ‘‹

**System Developer (Telecom) + Industrial PhD (SE/AI)** based in Sweden.  
I build and improve production systems, and I research **how to evaluate and verify LLM-generated artifacts** (text/code/structured specs) with **reproducible pipelines**.

- ğŸ’¼ Work: system development, performance/capacity, reliability, tooling & automation
- ğŸ“ Research: LLM output evaluation, validity gates, metrics, structured artifacts (e.g., requirements/UML)
- ğŸ¤ Open to: applied SE+AI collaboration, benchmarking/evaluation discussions, researchâ€“industry projects

**Links:**  
- GitHub: https://github.com/chi0600  
- Email: chi.xiao [at] ericsson.com  
- (Optional) Scholar: <link> Â· ORCID: <link> Â· Slides: <link>

---

## â­ Featured (Pin these on the profile)

### 1) `uml-llm-eval-2025` â€” Reproducible LLM Artifact Evaluation
A reproducible evaluation pipeline for LLM-generated UML/structured artifacts across models and domains.  
**What youâ€™ll find:** dataset curation Â· prompting (zero/one/few-shot) Â· metrics & error analysis Â· scripts/Docker

### 2) `<repo>` â€” Engineering / Tooling (System Dev)
A practical tool or automation that shows your engineering strength (CI, scripts, observability, tooling, etc.).  
**Why it matters:** clear input/output Â· easy to run Â· real impact

### 3) `<repo>` â€” RAG / Embeddings / Experiment Runner (Optional)
A lightweight project that demonstrates production-minded ML tooling: configs, reproducibility, evaluation.

---

## ğŸ§© What I Do (Work + Research)

### System Development
- Design and implement maintainable components and workflows
- Performance/capacity analysis and engineering trade-offs
- Reliability, observability, and automation (CI/scripts)

### Industrial PhD Research
- Artifact-level evaluation: validity gates, oracle/judge design, metrics
- Cross-domain evaluation pipelines for LLM outputs
- Reproducible experiments and reporting

---

## ğŸ“„ Publications / Research Outputs
- **UML Sequence Diagram Generation: A Multi-Model, Multi-Domain Evaluation** â€” ICSE-SEIP 2025  
  Artifact/Code: <link> Â· Preprint: <link> Â· Slides: <link>

- (Optional) Pipeline generalization / workshop follow-up  
  Status: <draft/submitted> Â· Materials: <link>

> Keep this section â€œverifiableâ€: every item ideally has a link (artifact/preprint/slides).

---

## ğŸ›  Tech Stack
**Python** Â· **Git** Â· **Docker** Â· **CI (GitHub Actions)** Â· **Linux** Â· **LaTeX**  
LLM tooling: RAG Â· embeddings Â· evaluation (metrics, datasets, experiment runners)

---

## ğŸ“Œ Highlights (Quick Scan)
- Reproducible evaluation pipelines (scripts/configs/Docker)
- Engineering mindset: clarity, maintainability, measurable impact
- Bridging industry needs and research methods
